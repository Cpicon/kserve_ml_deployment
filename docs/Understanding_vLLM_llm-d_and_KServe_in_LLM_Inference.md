Understanding vLLM, llm-d, and KServe in LLM Inference

Introduction

The landscape of serving large language models (LLMs) is rapidly evolving, with new tools emerging to tackle performance and scalability challenges. vLLM, llm-d, and KServe are three key projects at the forefront of this evolution, often used together in Kubernetes environments for high-performance LLM inference. In this guide, we’ll explain each technology term and how they relate, going deep into the technical details. We’ll see why vLLM is a critical component that llm-d and KServe rely on, and how these pieces integrate to serve LLMs efficiently at scale.

Context: vLLM is a fast LLM inference engine, llm-d is a Kubernetes-native distributed inference framework built on vLLM, and KServe is a model serving platform that now integrates vLLM and llm-d for easy deployment of LLMs. By combining these, you get the optimized performance of vLLM with the scalability of llm-d and the user-friendly orchestration of KServe ￼. We’ll break down each component and the acronyms/tech terms associated with them, then show how they work together.

vLLM – A High-Performance LLM Inference Engine

vLLM is an open-source library and server for LLM inference, originally developed at UC Berkeley (with roots in a 2023 research paper on PagedAttention memory management) ￼. The goal of vLLM is to make serving large models both fast and resource-efficient ￼. It addresses the unique challenges of LLM inference, such as huge memory usage, long generation times, and scaling difficulty ￼. Key technical innovations in vLLM include:
	•	PagedAttention – a smarter GPU memory management system for the transformer key-value (KV) cache. Instead of pre-allocating giant contiguous memory for all tokens (which leads to waste), vLLM breaks the cache into fixed-size “pages” and allocates memory on demand ￼ ￼. This is like virtual memory paging in operating systems, applied to LLM attention caches. It eliminates fragmentation and “memory hoarding,” allowing more of the GPU’s memory to be put to actual use ￼. The result is significantly less wasted memory and the ability to handle larger batches without running out of GPU RAM ￼. (This technique was shown to improve throughput by up to 24× vs. naive approaches in research results ￼.)
	•	Continuous batching – an advanced request batching system. Traditional serving frameworks process a batch of requests to completion before moving to the next batch, which can leave the GPU idle if some requests finish early. vLLM instead does dynamic batching: as soon as one request in a batch finishes (e.g. a short response), a new request can take its place in the next GPU iteration. This “sliding window” of requests means the GPU is continuously fed work, dramatically increasing throughput and reducing latency for new requests ￼. Essentially, vLLM behaves like a restaurant waiter who keeps taking new orders and filling empty spots on the tray instead of waiting for an entire batch of dishes to be done ￼.
	•	Efficient parallelism and optimized kernels – vLLM supports running on multiple GPUs (including tensor and pipeline parallelism for very large models) ￼. It comes with highly optimized CUDA kernels for transformers, and supports modern GPU features like NVIDIA’s FlashAttention algorithm (vLLM v0.10 adds full support for FlashAttention v3 and CUDA Graph capturing for efficient execution) as well as quantization (INT8/INT4, FP8 etc.) to speed up inference ￼. It also can use multiple threads and vectorized operations on CPUs. All these lower-level optimizations squeeze maximum performance out of the hardware.
	•	OpenAI-compatible API and broad model support – vLLM runs an HTTP server that implements the same REST API as OpenAI’s GPT endpoints (for completions, chat, embeddings, etc.) ￼ ￼. This makes it easy to integrate – you can point any existing OpenAI API client to a vLLM server. vLLM supports a “smorgasbord” of popular LLM architectures (LLaMA, Mistral, GPT-2/3, etc., even Mixture-of-Experts models) ￼. It also supports advanced features like speculative decoding (generating tokens using a draft model to speed up responses) ￼ and prefix caching. Prefix caching means that if multiple requests share the same initial prompt prefix, vLLM will reuse the cached key-value vectors instead of recomputing them ￼. This is especially useful for chat sessions or iterative prompts where each query builds on the last – vLLM can skip over the part it has already processed, greatly reducing latency for those cases ￼.

In summary, vLLM is a specialized engine that dramatically improves LLM serving speed and efficiency. It solves memory bloat and long latency via techniques like PagedAttention and continuous batching ￼ ￼. Because of these benefits, vLLM has been widely adopted – it’s open source (Apache 2.0) with a large community (40k+ GitHub stars) ￼, and is considered state-of-the-art for LLM inference performance.

How to use vLLM: You can use vLLM as a standalone server or library. For example, after installing vllm (e.g. via pip), you can launch an OpenAI-style API endpoint with a one-liner command. Below we start a vLLM server for a given model and then issue a chat completion request to it:

# Run vLLM to serve a model (downloads model if not local)
vllm serve "neuralmagic/granite-3.1-8b-instruct-quantized.w4a16"

# Once running (listening on http://localhost:8000 by default), 
# you can request a completion using the OpenAI API format:
curl -X POST "http://localhost:8000/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "neuralmagic/granite-3.1-8b-instruct-quantized.w4a16",
    "messages": [{"role": "user", "content": "What is the capital of France?"}]
  }'

The above will return a completion response JSON (e.g., with the assistant’s answer “Paris”). Under the hood, vLLM served this request efficiently by batching it with others (if any) and using its optimized attention mechanism. The OpenAI-compatible API means you could also use an OpenAI SDK or LangChain, pointing it to your vLLM endpoint, and it would work the same as calling OpenAI’s cloud API ￼ ￼.

llm-d – Distributed, Disaggregated Inference on Kubernetes

llm-d (short for “LLM, distributed”) is a Kubernetes-native framework that extends vLLM to work at cluster scale ￼ ￼. It was launched in 2025 as a community-driven project by contributors from Google, Red Hat, IBM, NVIDIA, and others, designed to bring Google-scale inference techniques to open source ￼ ￼. In essence, llm-d takes the single-node optimizations of vLLM and adds a layer of intelligent distribution, scheduling, and scaling, so that even the largest models or highest loads can be served efficiently across many GPUs.

How llm-d works: It introduces several architectural innovations on top of Kubernetes and vLLM:

llm-d architecture: The llm-d stack on Kubernetes consists of an Inference Gateway (a Kubernetes Gateway API extension) that fronts an Inference Scheduler service. The scheduler uses a custom “Endpoint Picker” to route each request to the optimal backend instance. Behind the gateway, multiple vLLM-based inference pods run as model servers. These are split into specialized groups, e.g. “Prefill” pods (handling the initial prompt processing) and “Decode” pods (handling iterative token generation), enabling disaggregated inference. A multi-tier KV cache (prefix cache) is shared across instances (in-memory or via external store) to maximize cache hits. This design allows llm-d to minimize latency (by reusing cached results and balancing load) and maximize throughput by scaling different parts of the workload independently ￼.
	•	Intelligent Scheduling (Inference Gateway & EPP): llm-d replaces naive load-balancing with a vLLM-aware scheduler ￼. Instead of round-robin or random distribution of requests, llm-d’s scheduler knows the state of each vLLM instance – including how busy it is and what data is in its cache. It uses an Endpoint Picker Protocol (EPP) to assign incoming requests to the best instance, e.g. one that already has the prompt’s prefix in cache (a prefix-cache hit) and that isn’t overloaded ￼. By increasing cache hits and avoiding busy servers, it achieves better latency SLOs with fewer resources ￼. The scheduler is implemented as part of an Inference Gateway (IGW), which extends Kubernetes’ Gateway API for L7 routing of inference requests. This means llm-d integrates with Kubernetes networking – you define an HTTPRoute for your model’s endpoint, and llm-d’s gateway will handle routing and balancing using the custom EPP logic. (This Gateway API Inference extension is often abbreviated GIE in community discussions.)
	•	Disaggregated Serving (Prefill/Decode split): llm-d introduces a disaggregated inference architecture for LLMs ￼ ￼. In a traditional setup, a single process handles the entire request – from reading the prompt (the “prefill” phase) to generating each token (the “decode” phase). However, these two phases have different characteristics: Prefill (processing the input prompt through the model) is compute-intensive but only runs until the first token is produced; Decode (iteratively generating output tokens) is memory- and bandwidth-intensive and can run for a long time, especially for lengthy responses ￼. Co-locating both phases on the same GPU can be inefficient – the GPU might be underutilized during decode if it’s optimized for the bursty prefill, or vice versa ￼. llm-d’s solution is to split these phases onto different groups of servers: e.g. one set of vLLM instances dedicated to prefill, and another set dedicated to decode ￼. The scheduler then coordinates so that a request’s prompt is processed by a prefill server, then its partial results (like the model’s internal state after the prompt) are transferred to a decode server to finish generating tokens. This way, each type of server can be optimized (and scaled) for its specific task – yielding faster time-to-first-token and more predictable token generation throughput ￼. Disaggregation also means you can scale out decode pods horizontally to handle many long-running outputs, without needing to replicate the heavy prompt computation on each one unnecessarily. (It’s similar in spirit to map-reduce: split a job into two stages that can be parallelized or tuned independently.)
	•	Prefix Cache and Multi-tier KV Storage: Since vLLM already implements prefix caching (automatic reuse of previous key/value tensors for repeated prefixes), llm-d amplifies this benefit at cluster scale. It introduces a pluggable KV cache layer ￼ that can share and offload these cache entries across the cluster. In practice, llm-d can run with an in-memory cache (each vLLM pod keeps its own cache, and the scheduler routes sticky requests to the same pod for cache hits – this is simpler) or a coordinated cache (where cache entries could be transferred between pods or stored in a shared external system so that any pod can retrieve them) ￼. The latter might use a global index or even an external KV store (like a Redis or the experimental LMCache system) to lookup cached keys. The llm-d team refers to “N/S offloading” (north/south, to local disk or host memory for cheap storage) vs “E/W sharing” (east/west, between peers or a central cache for performance) ￼. The immediate implementation in llm-d v0.2 focuses on precise prefix-cache aware routing – meaning it doesn’t require an external cache at first, it just routes requests cleverly so they hit the same pod’s cache, avoiding redundant computation ￼. This has been shown to drastically improve tail latency for multi-turn and iterative workloads, because if, say, a code-completion request series keeps reusing the same context, llm-d will send it to the pod that already holds that context in memory ￼ ￼.
	•	Autoscaling and Advanced Parallelism: llm-d is also working on sophisticated autoscaling for LLM workloads. Traditional autoscaling (via Kubernetes HPA) might not fit well because LLM requests are long and resource-intensive in varying ways. The project is exploring “variant autoscaling”, where it can scale prefill vs decode pods based on real-time load and performance metrics (e.g., queue lengths, token processing rates) ￼. They are even looking at predictive autoscaling using ideas from the research project “Inferno” to proactively scale for incoming workload patterns. On the parallelism front, llm-d’s design can accommodate data-parallel and expert-parallel scaling for giant models (like 200B+ parameter or Mixture-of-Experts models). In fact, one of their “well-lit paths” is Wide Expert-Parallelism which coordinates scaling MoE models across many GPUs using vLLM’s support for such models ￼ ￼. This means llm-d could manage a model that is sharded by experts or layers across multiple nodes, not just replicate entire models. All of this is geared toward serving the largest models (think multi-node distributed inference) efficiently.

In simpler terms, llm-d turns a collection of vLLM servers into a smart distributed inference cluster. It layers on a control plane (scheduler, gateway) that is LLM-aware, ensuring each query gets to the right place and that the whole system scales effectively. By leveraging Kubernetes, it treats these components (gateway, scheduler, model pods) as Kubernetes objects (using CRDs like InferenceService, InferencePool under the Gateway API Inference Extension) so that integration with cloud-native environments is seamless.

Importantly, llm-d is built on vLLM as its execution engine. Each “inference pod” in llm-d is basically running vLLM (with some configuration for whether it’s doing prefill or decode). The tight collaboration between the llm-d team and vLLM means new vLLM features (like support for disaggregated mode, or hooks for custom caching) are incorporated to enable llm-d’s functionality ￼. For example, vLLM introduced a KV connector API that llm-d uses to manage cache entries across instances. Because of this design, vLLM is a dependency of llm-d – without vLLM’s fast generation core, llm-d wouldn’t have a high-performance backend to schedule. Conversely, llm-d extends vLLM to use cases (multi-node, multi-instance scenarios) that vLLM alone didn’t previously handle easily, making the two highly complementary.

KServe – Kubernetes Model Serving Meets Generative AI

KServe is an open-source model serving platform built for Kubernetes (it originated as KFServing in the Kubeflow project). Its mission is to make deploying machine learning models to production as easy as writing a simple YAML file. Under the hood, KServe provides a Kubernetes control plane that handles provisioning of model server pods, networking (ingress/gateways), autoscaling, and health management, using a serverless paradigm (it can scale your model pods to zero when idle, and back up on demand).

Traditionally, KServe has supported serving “predictive” models (like scikit-learn, XGBoost, TensorFlow, etc.) with various pluggable runtimes. As Generative AI and large models rose to prominence, KServe has adapted to serve these LLM workloads too. Instead of expecting users to manage custom deployments, KServe integrated vLLM as a built-in backend for text generation tasks ￼. Specifically, KServe offers a Hugging Face model server runtime: if you specify your model is a HuggingFace transformer (like a GPT or T5), KServe will by default launch a predictor using vLLM under the hood ￼. This provides significant performance benefits: “faster time-to-first-token, higher token throughput, and better memory efficiency” compared to naive approaches ￼. In other words, KServe automatically chooses the best engine (vLLM) for LLMs as part of its pluggable architecture ￼.

How KServe uses vLLM: The KServe controller abstracts away the complexity of running vLLM. A user simply creates an InferenceService YAML with modelFormat: huggingface and points to the model (e.g. a HuggingFace Hub model name or a PVC path). When applied, KServe will spawn a predictor pod that hosts a HuggingFace transformer server. In KServe v0.15+, this server will initialize vLLM if the model is a text generation or text-to-text task, otherwise it can fall back to the native transformers pipeline ￼ ￼. The predictor exposes a standardized HTTP endpoint (KServe’s data-plane v2 protocol, and also the OpenAI-compatible endpoints for chat/completions as supported by vLLM ￼). The networking is set up via Istio or Kubernetes Gateway API (KServe v0.15 added support for Gateway API as well) so that you get a URL to call the model. KServe also mounts model weights automatically (from storageUri which can be a cloud bucket, PVC, HuggingFace Hub reference, etc.), so you don’t have to manually load the model – it’s handled by an initializer or sidecar container.

For example, here’s a KServe InferenceService spec that deploys a Llama-3 8B model using the HuggingFace+vLLM runtime:

apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: huggingface-llm
spec:
  predictor:
    model:
      modelFormat:
        name: huggingface          # Use HuggingFace model server (will use vLLM by default)
      storageUri: "hf://meta-llama/llama-3-8b-instruct"  # Model on HuggingFace Hub
      resources:
        limits:
          nvidia.com/gpu: 1        # Request a GPU, KServe will choose the CUDA-enabled vLLM image
        requests:
          nvidia.com/gpu: 1

In this YAML, we specify the model repository (hf://...), and request a GPU. KServe will pull an appropriate container image that includes vLLM (the “CUDA-enabled” HuggingFace runtime image) ￼ ￼, load the model, and serve it. The user can then simply do kubectl get inferenceservice huggingface-llm -n <namespace> to find the URL, and call the endpoint (which supports /v1/completions, /v1/chat/completions, etc., just like a standard OpenAI API) ￼. KServe thus makes using vLLM easier by handling deployment and scaling via familiar K8s tooling.

KServe + llm-d integration: While KServe could already deploy a single vLLM server, what about the advanced distributed serving that llm-d provides? The latest developments (as of 2025) are bridging KServe with llm-d. The community has introduced a new KServe custom resource called (tentatively) LLMInferenceService – essentially, a higher-level CRD to manage a collection of components for one logical LLM service. When a user creates an LLMInferenceService, the KServe controllers will spawn not just one predictor pod, but a whole llm-d deployment: it can create the Inference Gateway (GIE) with an Endpoint Picker configured, an InferencePool of vLLM pods (possibly split into prefill/decode roles), and any needed routing objects (like HTTPRoute for the Gateway) – all behind the scenes. This gives users the best of both worlds: the “production-ready control plane” features of KServe (easy scaling, configuration, and integration with the rest of your platform) combined with the powerful “disaggregated inference” capabilities of llm-d ￼. In fact, the llm-d project considers KServe integration important enough that a GitHub feature request explicitly calls for combining llm-d with KServe for exactly these benefits ￼.

Recent updates from the Working Group Serving community indicate progress on this integration. They have implemented reconciliation logic in KServe for multi-component resources like the Gateway API’s HTTPRoute (to wire up network traffic to the llm-d scheduler) and merging of an LLMInferenceServiceConfig (allowing unified config of all sub-components). They’ve also added support for different storage backends (S3, PVC, OCI) in LLMInferenceService, so models can be loaded from various sources just like regular KServe models. Notably, they have deployed the GIE Gateway with EPP and an InferencePool in a production-like setting to test it, and worked on model discoverability (so the service can report what model/variants are running) and end-to-end encryption of traffic between the gateway, scheduler, and vLLM pods (using mTLS perhaps, to secure in-cluster communications). All of this points toward a near-future where you can simply declare something like:

apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  name: my-llama-service
spec:
  model: 
    name: llama-70b
    storage: s3://mybucket/llama-2-70b-hf
  inferencingStrategy: "PrefillDecodeDisaggregated"
  gateway:
    route: PublicEndpoint  # expose via Gateway
    tls: EncryptTraffic
  # (This is a pseudo-spec just for illustration)

…and behind the scenes, KServe will stand up the llm-d infrastructure to serve the 70B model with multiple pods and intelligent routing. (The actual schema is evolving, but conceptually this is what’s happening.)

In KServe v0.15, some pieces of this were already in motion: they introduced Multi-Node Inference support to handle models too large for one node (using pipeline parallelism across GPUs) ￼, and hinted at a “new distributed inference API” to support disaggregated prefilling ￼. They also integrated with external KV cache systems like LMCache (a specialized library to share cache across processes) as an alternative approach to improving LLM serving latency ￼ ￼. These efforts run in parallel with llm-d; going forward, we expect llm-d’s approach (with native vLLM cache sharing) to be one path, and things like LMCache to be another – KServe is likely to support multiple options to optimize LLM inference.

How They Relate and Why vLLM Is Central

To summarize the relationships:
	•	vLLM is the core engine that actually runs the model inference. It’s used by both llm-d and KServe because it provides unparalleled performance for LLM generation (thanks to features like PagedAttention and continuous batching) and a convenient API. For example, KServe explicitly uses vLLM as the default backend for transformer models to achieve better latency and throughput ￼. llm-d, on the other hand, leverages vLLM within each of its distributed workers – without vLLM’s support for things like prefix caching and distributed execution, llm-d would not be nearly as effective. In short, vLLM is the foundation upon which the others build. It allows llm-d and KServe to focus on higher-level orchestration, instead of reinventing the low-level inference kernel.
	•	llm-d extends vLLM to multiple pods/nodes and adds a smarter routing layer. You can think of llm-d as an orchestration and optimization layer for vLLM. It is not a replacement for vLLM; rather, it’s an add-on that uses many vLLM instances in concert. The designers of llm-d chose vLLM because it was (and continues to be) one of the fastest inference servers available, open-source, and already capable of some forms of distributed inference (vLLM has hooks for tensor parallelism and such) ￼. vLLM’s architecture (especially the PagedAttention KV cache) is what enables llm-d’s cache-aware scheduling – for example, vLLM keeps the KV cache in memory in a way that can be queried for hits, and llm-d takes advantage of that. By using vLLM, llm-d immediately got a ~24x throughput advantage over naive implementations ￼ and avoided redundant computation through cache reuse ￼. This synergy allows llm-d to claim “fastest time-to-value and performance per dollar” for large model serving ￼, since it builds on the best-of-breed engine.
	•	KServe integrates with both: initially with vLLM directly, and increasingly with llm-d for advanced scenarios. KServe’s role is to make deployment and management easy. It treats vLLM and llm-d as pluggable backends (runtimes). In fact, KServe’s pluggable architecture was explicitly designed so that users can choose “the best inference engine for your model type” ￼ – and for LLMs, vLLM was chosen due to its optimizations. Now with llm-d, KServe aims to expose those distributed capabilities without exposing the complexity. The benefit for KServe users is huge: you can scale LLM inference to many GPUs and handle tricky workloads, but still manage it with KServe’s simple declarative interface. Meanwhile, llm-d benefits from KServe by getting a ready-made control plane (for things like authentication, logging, multi-tenancy, etc.) and easier adoption (since many companies already use KServe in production). As the llm-d team put it, “combining llm-d with KServe, you gain production-ready control plane features along with the capabilities of llm-d.” ￼.

Why vLLM? Both llm-d and KServe could have tried to use other inference solutions (like Hugging Face’s Text Generation Inference or NVIDIA’s FasterTransformer). They chose vLLM largely because of its performance and compatibility. vLLM demonstrated order-of-magnitude gains in throughput and memory use ￼, which is crucial when serving costly models – it directly translates to lower infrastructure cost and better user experience. It also supports the OpenAI API and many model types out-of-the-box, reducing integration effort. Moreover, vLLM is continuously updated to support new hardware (for example, v0.10 added support for NVIDIA’s next-gen Blackwell GPUs with FP8/INT8 acceleration, and support for AMD MI300 and even TPU) and new model features (like MoE inference, streaming, etc.), so it future-proofs the serving stack. In community updates, we see that vLLM 0.9.2/0.10.0 introduced features like full CUDA Graph execution for entire sequences, priority scheduling for requests, and support for a plethora of new model architectures (from Llama-2 to Mistral, Qwen, etc.) – this pace of innovation is hard to match ￼ ￼. By aligning with vLLM, projects like llm-d and KServe ride that wave of innovation instead of playing catch-up.

Another reason is architectural alignment: vLLM was designed with server use in mind (it’s not just a PyTorch model code; it’s an actual serving system with cache management, multi-request handling, etc.). That makes it relatively straightforward to integrate into a multi-instance architecture. vLLM’s developers and llm-d’s developers even collaborate (the llm-d project is co-designed with vLLM’s capabilities in mind ￼). For KServe, using vLLM meant they could immediately offer things like streaming token responses, which would have been very hard with a generic Transformers backend.

In summary, vLLM is the engine, llm-d is the smart traffic cop and plumber, and KServe is the user-friendly storefront. When a user sends a request to a KServe-managed endpoint, it might go through an llm-d Gateway (with vLLM-aware scheduling) and land on a vLLM server process which then generates the result. The combination is Kubernetes-native and highly optimized for LLM workloads, tackling the “hard parts” of LLM serving: huge memory footprints, long and variable latency, and scaling across many GPUs. All three are open-source and part of an ecosystem pushing the state-of-the-art in LLM deployment.

Example Usage and Integration

Finally, to solidify understanding, let’s look at brief examples of how each component is used and how they integrate:
	•	Using vLLM by itself: We showed earlier how to launch vLLM’s server and query it with curl. You can also use the vLLM Python API directly in code (for advanced use cases outside of HTTP). But typically, one would run vllm serve in a container or VM to create an endpoint. Many projects (like FastChat, LangChain, etc.) can be configured to use a vLLM endpoint for high-performance local inference.
	•	Deploying a model with KServe (using vLLM): Suppose you want to serve a model on Kubernetes. With KServe installed, you can simply apply an InferenceService manifest as shown above. Once the service is Ready, you could invoke it. For example, if you have a load-balancer ingress, you might do:

export SERVICE_HOST=$(kubectl get inferenceservice huggingface-llm -o jsonpath='{.status.url}')
curl -X POST "$SERVICE_HOST/v1/completions" -d '{
  "model": "meta-llama/llama-3-8b-instruct",
  "prompt": "Once upon a time",
  "max_tokens": 50
}'

This request goes to the KServe ingress (which could be backed by Istio or Gateway API). The request will reach the predictor pod running vLLM, which will generate a completion. KServe abstracts the infrastructure – you didn’t have to manually start vLLM or worry about scaling it; KServe will monitor load and can auto-scale replicas if needed (including scaling to 0 on idle if configured with Knative). Moreover, if the model wasn’t already downloaded, KServe’s storage initializer would have pulled it from HuggingFace to a local path (/mnt/models volume) before vLLM started.

	•	Deploying llm-d on Kubernetes: llm-d provides Helm charts and kustomize configs as part of its “well-lit paths” examples ￼. To deploy llm-d, you typically add the llm-d Helm repository and install a chart with parameters for your model and desired topology. For instance, you might do something like:

# (Pseudo-commands for illustration)
helm repo add llm-d https://llm-d.ai/charts
helm install my-llmd llm-d/llm-d \
     --set model.name="Llama-2-70b-chat-hf" \
     --set prefill.replicas=2,decode.replicas=4 \
     --set gateway.endpointType=LoadBalancer 

This would deploy the llm-d components: an Inference Gateway service (exposed as a LoadBalancer), a scheduler, and the vLLM pods for prefill and decode (2 of the former, 4 of the latter in this example). In practice, you’d also provide configuration like model storage URIs or Kubernetes PVCs where the model is stored, and possibly accelerator-specific settings. Once deployed, you can send requests to the gateway’s endpoint. The gateway in llm-d currently speaks a similar protocol to vLLM’s OpenAI API for completions (since it ultimately dispatches to vLLM). For example, if the gateway IP is $GATEWAY_IP:

curl -X POST "http://$GATEWAY_IP/completions" -H "Content-Type: application/json" -d '{
  "prompt": "Q: What is Kubernetes?\nA:",
  "max_tokens": 100
}'

The scheduler will receive this request, determine which vLLM pod (prefill & decode) should handle it (taking into account cache and load), and route it accordingly. The response is then returned via the gateway. To the caller, it looks like a single service. In the background, llm-d ensured that if this question was asked before, it hits a warm cache, or if it’s a long prompt, it gets split across specialized instances, etc.
If using KServe’s upcoming LLMInferenceService, the user might not run helm themselves – instead, they create the CR and KServe’s controller would effectively do the above for them (possibly by creating the necessary Kubernetes objects). The integration is still evolving, but the ultimate goal is that using llm-d through KServe will be as simple as any other KServe deployment.

By understanding each layer – vLLM as the optimized engine, llm-d as the distributed scheduler leveraging that engine, and KServe as the user-facing deployment interface – an engineer can design an LLM serving solution that fits their needs. For instance, a single GPU setup might just use vLLM directly or via a basic KServe InferenceService. A larger deployment requiring minimal latency for repeated queries might use KServe with llm-d to take advantage of cache-aware routing and split decoding. All components are modular: you could even run llm-d without KServe (managing it via Helm/Argo CD, etc.), or run KServe without llm-d (for smaller models), depending on your scenario.

In summary, vLLM, llm-d, and KServe form a synergistic stack:
	•	vLLM provides the fast inference core (efficient GPU usage, fast token generation).
	•	llm-d provides the distributed serving brain, orchestrating multiple vLLM instances to act as one logical service with intelligent request routing and scaling ￼.
	•	KServe provides the Kubernetes-native facade to deploy and manage these services easily, tying in with ingress, autoscalers, and developer-friendly abstractions ￼ ￼.

Each term we discussed – from PagedAttention (vLLM’s memory system) to prefill/decode disaggregation (llm-d’s split architecture) to InferenceService CRDs (KServe’s API objects) – plays a role in this ecosystem. By expanding and integrating these technologies, the community is navigating “the rapid evolution of large model inference” (as the KubeCon talk title suggests) and finding where Kubernetes fits in. And as we’ve seen, Kubernetes (via KServe and Gateway API) is being adapted to handle LLM-specific needs like long-lived requests, streaming responses, and specialized hardware scheduling.

Engineers delving into this should now have a clearer mental model of how these pieces fit: when you send a prompt to a well-architected LLM service, there might be a gateway doing smart routing (llm-d), a set of servers crunching numbers with maximal efficiency (vLLM), all deployed with a few Kubernetes YAMLs (KServe). Each layer abstracts the next, but understanding them allows you to tune and troubleshoot your LLM deployments for the best performance and cost-efficiency.

Sources: The information above is drawn from the official docs and community updates of each project. For instance, vLLM’s capabilities are documented in its introduction and blogs ￼ ￼, llm-d’s design is described in its announcement and README ￼ ￼, and KServe’s LLM integration is outlined in its documentation and release notes ￼ ￼. These sources (cited throughout) provide further reading for those who want to dive even deeper into any specific term or component.